---
title: "Spatial Autocorrelation"
author: "Me"
date: "November 4, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Much of spatial analysis deals with understanding how objects or attributes are clustered together. In general, we consider random distributions, uniform distributions, and clustered distributions to be the focus of our analysis. The statistics used to analyze these clusters are therefore, trying to tell us how closely related objects are in space. 
When dealing with spatial data we must also considered that objects next to each other may have influence on one another. That is, objects in space are not, in fact indepdent, but are rather dependent and the intensity of dependencies varies by how close two objects or neighborhoods are to each other. The statistics must reflect this dependency and are the proccess of relating clustered values together is known as spatial autocorrelation. 
First we must understand what statistics we will want to use in order to spatial autocorrelation. We will go over

*Join Count Analysis 
*Moran's I
*Geary's C
*Getis Ord Gi and Gi star

We need a few packages
```{r}
library(curl)
library(ape) #you may need to install this package first
library(spdep) #for spatial autocorrelation
library(ncf)
library(raster)
library(pgirmess) #to create your raster grid cells
```

Let us start with join count analysis. This type of analysis looks at sets of data that have character attributes and spatial location. Character attributes can include prescence/abscence data as well as characterizations such blue versus red states. Join count statistics can tell you whether your data is clumped, randomly dispersed, or uniformly dispersed. Join count statistics are typically used when dealing with polygons rather than points. 

First we need some data. We will use tree data from Harvard to start and then apply this methodology to our own data that we created. We will look at the distributions of three different tree species. 

```{r}
HTrees <- read.csv("http://harvardforest.fas.harvard.edu/data/p03/hf032/hf032-01-tree.csv")
```

Next we need to create a raster for our data. A raster is essentially a grid, which is made up of cells. Cells can be any size that you define and can impact the spatial resolution of your data. Be careful when you define your raster size as the size may be dependent on the type of data you have and the data limitations. 

```{r}
# Create subset of data for Acer rubrum
acru <- subset(HTrees, species == "ACRU" & dbh91 != "NA", select = c("xsite", "ysite", "dbh91"))
# Create subset of data for Prunus serotina
prse <- subset(HTrees, species == "PRSE" & dbh91 != "NA", select = c("xsite", "ysite", "dbh91"))
# Create subset of data for Pinus strobus
pist <- subset(HTrees, species == "PIST" & dbh91 != "NA", select = c("xsite", "ysite", "dbh91"))

# Convert tree data to SpatialPointsDataFrame, both for entire dataset, and # for individual species
HTrees.spdf <- HTrees
coordinates(HTrees.spdf) <- c("xsite", "ysite")
pist.spdf <- pist
coordinates(pist.spdf) <- c("xsite", "ysite")
acru.spdf <- acru
coordinates(acru.spdf) <- c("xsite", "ysite")
prse.spdf <- prse
coordinates(prse.spdf) <- c("xsite", "ysite")

#Define the extent for the join count analyses
jc.extent <- extent(-300,100,-700,-200)
#set up a blank raster
r <- raster(nrows=25, ncols=20, ext=jc.extent)

acru.rast<-rasterize(acru.spdf, r, field = 1)
acru.rast[is.na(acru.rast)] <- 0
# You can plot the result
plot(acru.rast)
# You can plot the points on top of the raster to verify it is correct
# Remember - we only mae the raster for a subset of the data, so there will # be points in a larger area than the raster covers
plot(acru.spdf, add = TRUE)

# Generate neighbors list - the function is 'cell2nb' and the arguments are
# the number of rows and colums in your grid; you can simply get those
# characteristics from your any of your rasters using 'nrow' and 'ncol'
# commands, nested in the cell2nb function (as ilustrated below). Note, the
# default for this is 'rook', but you can change the join counts to 'queen'
# by adding the argument 'type='queen'.
nb <- cell2nb(nrow = nrow(acru.rast), ncol = ncol(acru.rast))
# To calculate neighbors for queen configuration
nb.queen <- cell2nb(nrow = nrow(acru.rast), ncol = ncol(acru.rast), type = "queen")
# Convert the neighbors list to a 'weights' list; again, this will be the
# same for all species we are analyzing. You an follow the example below
# using 'style='B' (as a Binary weights matrix). Again, calculate this for
# the queen setup as well as the default (rook) setup.
lwb <- nb2listw(nb, style = "B")
lwb.queen <- nb2listw(nb.queen, style = "B")

# First, the regular join count test for Acer rubrum (Testing the hypothesis
# of aggregation among like categories; add the argument 'alternative='less'
# to reverse this)
joincount.test(as.factor(acru.rast@data@values), lwb, alternative = "greater") # Second, the permutation-based jon count test; similar to above, and you
# can adjust the number of simulations with the 'nsim' argument
joincount.mc(as.factor(acru.rast@data@values), lwb, nsim = 999, alternative = "greater")
# Can also compute these for the queen setup; for example, with the
# permutation test:
joincount.mc(as.factor(acru.rast@data@values), lwb.queen, nsim = 999, alternative = "greater")
```
Now lets break this methodology down so that we can apply it to our own data for join count statistics. 


